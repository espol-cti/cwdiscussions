{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying type of opinions in spanish wikipedia discussions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we are going to automatically identify the kind of opinion of authors in the discussions on talk pages of spanish wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from wdds_tokenizer import tokenize\n",
    "import wdds_tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#matplotlib.style.use('seaborn-ticks')\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['ytick.labelsize'] = 14\n",
    "matplotlib.rcParams['xtick.labelsize'] = 14\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "matplotlib.rcParams['axes.titlesize'] = 18\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our labelled dataset, containing the opinions in the talk pages of wikipedia segmented by sentences assuming normal punctuation. This dataset includes the initial 1000 edits of talk pages of political leaders in America."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2097, 18)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('data/wdds.csv')\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1579, 19)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds[~ds.type.isnull()]\n",
    "ds = ds[ds.subtype!='INVALID']\n",
    "ds = ds[ds.subtype!='SIGN']\n",
    "ds = ds[ds.subtype!='OLAN']\n",
    "\n",
    "ds['target'] = ds['type']\n",
    "ds['opinion'] = ds['clean_opinion']\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 1026\n",
      "test size: 553\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.35, random_state=0)\n",
    "sss.get_n_splits(ds.opinion, ds.target)\n",
    "\n",
    "for train_index, test_index in sss.split(ds.opinion, ds.target):\n",
    "   X_train, X_test = ds.iloc[train_index].opinion, ds.iloc[test_index].opinion\n",
    "   y_train, y_test = ds.iloc[train_index].target, ds.iloc[test_index].target\n",
    "\n",
    "print(f'train size: {X_train.shape[0]}')\n",
    "print(f'test size: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_vectorizer = [\n",
    "    ('vect', CountVectorizer(strip_accents='ascii', \n",
    "                             min_df=3, max_df=0.8,\n",
    "                             stop_words=wdds_tokenizer.stopset, \n",
    "                             tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True, sublinear_tf=True))\n",
    "]\n",
    "\n",
    "multinb_clf = Pipeline(text_vectorizer+[('clf', MultinomialNB())])\n",
    "lsvc_clf = Pipeline(text_vectorizer+[('clf', LinearSVC())])\n",
    "mf_clf = Pipeline(text_vectorizer+[('clf', DummyClassifier(strategy='most_frequent', random_state=0))])\n",
    "uniform_clf = Pipeline(text_vectorizer+[('clf', DummyClassifier(strategy='uniform', random_state=0))])\n",
    "strat_clf = Pipeline(text_vectorizer+[('clf', DummyClassifier(strategy='stratified', random_state=0))])\n",
    "use_stemmer = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RND Accuracy: 0.33 (+/- 0.05)\n",
      "LSVC best score: 0.621832358674464\n",
      "LSVC best params: {'clf__C': 1, 'clf__tol': 0.001}\n",
      "LSVC Accuracy: 0.62 (+/- 0.06)\n",
      "MNB best score: 0.6023391812865497\n",
      "MNB best params: {'clf__alpha': 1.0, 'clf__fit_prior': True}\n",
      "MNB Accuracy: 0.60 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "summary_scores = ['CI (95\\%)', '']\n",
    "\n",
    "uniform_clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(uniform_clf, X_train, y_train)\n",
    "confidence_interval = scores.std() * 2\n",
    "print(\"RND Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), confidence_interval))\n",
    "summary_scores.extend([confidence_interval, '', ''])\n",
    "\n",
    "#parameters = {'clf__C':[0.1, 1, 10]}\n",
    "parameters = {'clf__C':[1]}\n",
    "parameters['clf__loss']=('hinge','squared_hinge')\n",
    "parameters['clf__multi_class']= ('ovr', 'crammer_singer')\n",
    "parameters['clf__class_weight'] = (None, 'balanced')\n",
    "parameters['clf__tol'] = [1e-3, 1e-4]\n",
    "lsvc_cv = GridSearchCV(lsvc_clf, parameters)\n",
    "lsvc_cv.fit(X_train, y_train)\n",
    "scores = cross_val_score(lsvc_cv, X_train, y_train)\n",
    "confidence_interval = scores.std() * 2\n",
    "print(f\"LSVC best score: {lsvc_cv.best_score_}\")\n",
    "print(f\"LSVC best params: {lsvc_cv.best_params_}\")\n",
    "print(\"LSVC Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), confidence_interval))\n",
    "summary_scores.extend([confidence_interval, '',''])\n",
    "\n",
    "parameters = {'clf__alpha':[ 0.1, 1.0, 10.0]}\n",
    "parameters['clf__fit_prior']= [True, False]\n",
    "multinb_cv = GridSearchCV(multinb_clf, parameters)\n",
    "multinb_cv.fit(X_train, y_train)\n",
    "scores =  cross_val_score(multinb_cv, X_train, y_train)\n",
    "confidence_interval = scores.std() * 2\n",
    "print(f\"MNB best score: {multinb_cv.best_score_}\")\n",
    "print(f\"MNB best params: {multinb_cv.best_params_}\")\n",
    "print(\"MNB Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), confidence_interval))\n",
    "summary_scores.extend([confidence_interval, '',''])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Support</th>\n",
       "      <th>BL-P</th>\n",
       "      <th>BL-R</th>\n",
       "      <th>BL-F1</th>\n",
       "      <th>LSVC-P</th>\n",
       "      <th>LSVC-R</th>\n",
       "      <th>LSVC-F1</th>\n",
       "      <th>MNB-P</th>\n",
       "      <th>MNB-R</th>\n",
       "      <th>MNB-F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARGUMENTATIVE</td>\n",
       "      <td>284</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.330986</td>\n",
       "      <td>0.393305</td>\n",
       "      <td>0.711340</td>\n",
       "      <td>0.728873</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.84507</td>\n",
       "      <td>0.722892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INTERPERSONAL</td>\n",
       "      <td>52</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.142259</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.778947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.790698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PERFORMATIVE</td>\n",
       "      <td>217</td>\n",
       "      <td>0.337209</td>\n",
       "      <td>0.267281</td>\n",
       "      <td>0.298201</td>\n",
       "      <td>0.639269</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.642202</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>0.414747</td>\n",
       "      <td>0.505618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Macro</td>\n",
       "      <td>553</td>\n",
       "      <td>0.304218</td>\n",
       "      <td>0.308397</td>\n",
       "      <td>0.277922</td>\n",
       "      <td>0.737025</td>\n",
       "      <td>0.695191</td>\n",
       "      <td>0.713716</td>\n",
       "      <td>0.759687</td>\n",
       "      <td>0.637888</td>\n",
       "      <td>0.673069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CI (95\\%)</td>\n",
       "      <td></td>\n",
       "      <td>0.052739</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.057428</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.017915</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Label Support      BL-P      BL-R     BL-F1    LSVC-P    LSVC-R  \\\n",
       "0  ARGUMENTATIVE     284  0.484536  0.330986  0.393305  0.711340  0.728873   \n",
       "1  INTERPERSONAL      52  0.090909  0.326923  0.142259  0.860465  0.711538   \n",
       "2   PERFORMATIVE     217  0.337209  0.267281  0.298201  0.639269  0.645161   \n",
       "3          Macro     553  0.304218  0.308397  0.277922  0.737025  0.695191   \n",
       "4      CI (95\\%)          0.052739                      0.057428             \n",
       "\n",
       "    LSVC-F1     MNB-P     MNB-R    MNB-F1  \n",
       "0      0.72  0.631579   0.84507  0.722892  \n",
       "1  0.778947  1.000000  0.653846  0.790698  \n",
       "2  0.642202  0.647482  0.414747  0.505618  \n",
       "3  0.713716  0.759687  0.637888  0.673069  \n",
       "4            0.017915                      "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "docs_test = X_test\n",
    "labels = y_test.unique()\n",
    "labels.sort()\n",
    "rds = pd.DataFrame({'Label': labels})\n",
    "macro_results = ['Macro', len(y_test)]\n",
    "\n",
    "predicted = uniform_clf.predict(docs_test)\n",
    "results = precision_recall_fscore_support(y_test, predicted)\n",
    "macro_results.extend(precision_recall_fscore_support(y_test, predicted, average='macro')[:3])\n",
    "rds['Support'] = results[3]\n",
    "rds['BL-P'] = results[0]\n",
    "rds['BL-R'] = results[1]\n",
    "rds['BL-F1'] = results[2]\n",
    "\n",
    "predicted = lsvc_cv.predict(docs_test)\n",
    "results = precision_recall_fscore_support(y_test, predicted)\n",
    "macro_results.extend(precision_recall_fscore_support(y_test, predicted, average='macro')[:3])\n",
    "rds['LSVC-P'] = results[0]\n",
    "rds['LSVC-R'] = results[1]\n",
    "rds['LSVC-F1'] = results[2]\n",
    "\n",
    "predicted = multinb_cv.predict(docs_test) \n",
    "results = precision_recall_fscore_support(y_test, predicted)\n",
    "macro_results.extend(precision_recall_fscore_support(y_test, predicted, average='macro')[:3])\n",
    "rds['MNB-P'] = results[0]\n",
    "rds['MNB-R'] = results[1]\n",
    "rds['MNB-F1'] = results[2]\n",
    "\n",
    "rds.loc[len(rds)]=macro_results\n",
    "rds.loc[len(rds)]=summary_scores\n",
    "\n",
    "rds.to_csv('output/classif_report.csv', index=False)\n",
    "rds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[207,   3,  74],\n",
       "       [ 10,  37,   5],\n",
       "       [ 74,   3, 140]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predicted = lsvc_cv.predict(docs_test)\n",
    "confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'y existen páginas específicas, foros por ejemplo, para exponer las opiniones sin más.' => ARGUMENTATIVE, PERFORMATIVE\n",
      "'realmente no coincido contigo en tu argumento, pues creo que en aquel caso los motivos eran también evidentes, pero no vamos a retomar esa discusión.' => ARGUMENTATIVE, ARGUMENTATIVE\n"
     ]
    }
   ],
   "source": [
    "sample = X_test[:2]\n",
    "labels = y_test[:2]\n",
    "\n",
    "docs_new = sample\n",
    "predicted = lsvc_cv.predict(docs_new)\n",
    "\n",
    "for doc, label, pred in zip(docs_new, labels, predicted):\n",
    "    print('%r => %s, %s' % (doc, label, pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
