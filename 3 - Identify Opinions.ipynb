{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying type of opinions in spanish wikipedia discussions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we are going to automatically identify the kind of opinion of authors in the discussions on talk pages of spanish wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from wdds_tokenizer import tokenize\n",
    "import wdds_tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#matplotlib.style.use('seaborn-ticks')\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['ytick.labelsize'] = 14\n",
    "matplotlib.rcParams['xtick.labelsize'] = 14\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "matplotlib.rcParams['axes.titlesize'] = 18\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our labelled dataset, containing the opinions in the talk pages of wikipedia segmented by sentences assuming normal punctuation. This dataset includes the initial 1000 edits of talk pages of political leaders in America."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2097, 20)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('data/wdds.csv')\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1583, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds[~ds.type.isnull()]\n",
    "ds = ds[ds.subtype!='INVALID']\n",
    "ds = ds[ds.subtype!='SIGN']\n",
    "ds = ds[ds.subtype!='OLAN']\n",
    "\n",
    "ds['target'] = ds['type']\n",
    "ds['opinion'] = ds['clean_opinion']\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 949\n",
      "test size: 634\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=0)\n",
    "sss.get_n_splits(ds.opinion, ds.target)\n",
    "\n",
    "for train_index, test_index in sss.split(ds.opinion, ds.target):\n",
    "   X_train, X_test = ds.iloc[train_index].opinion, ds.iloc[test_index].opinion\n",
    "   y_train, y_test = ds.iloc[train_index].target, ds.iloc[test_index].target\n",
    "\n",
    "print(f'train size: {X_train.shape[0]}')\n",
    "print(f'test size: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_vectorizer = [\n",
    "    ('vect', CountVectorizer(strip_accents='ascii', \n",
    "                             min_df=3, max_df=0.8,\n",
    "                             stop_words=wdds_tokenizer.stopset, \n",
    "                             tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True, sublinear_tf=True))\n",
    "]\n",
    "\n",
    "multinb_clf = Pipeline(text_vectorizer+[('clf', MultinomialNB())])\n",
    "lsvc_clf = Pipeline(text_vectorizer+[('clf', LinearSVC())])\n",
    "mf_clf = Pipeline(text_vectorizer+[('clf', DummyClassifier(strategy='most_frequent', random_state=0))])\n",
    "uniform_clf = Pipeline(text_vectorizer+[('clf', DummyClassifier(strategy='uniform', random_state=0))])\n",
    "strat_clf = Pipeline(text_vectorizer+[('clf', DummyClassifier(strategy='stratified', random_state=0))])\n",
    "use_stemmer = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RND Accuracy: 0.30 (+/- 0.01)\n",
      "LSVC best score: 0.6164383561643836\n",
      "LSVC best params: {'clf__C': 1, 'clf__class_weight': None, 'clf__loss': 'squared_hinge', 'clf__multi_class': 'ovr', 'clf__tol': 0.001}\n",
      "LSVC Accuracy: 0.62 (+/- 0.02)\n",
      "MNB best score: 0.5879873551106428\n",
      "MNB best params: {'clf__alpha': 0.1, 'clf__fit_prior': True}\n",
      "MNB Accuracy: 0.58 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "summary_scores = ['CI (95\\%)', '']\n",
    "\n",
    "uniform_clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(uniform_clf, X_train, y_train)\n",
    "confidence_interval = scores.std() * 2\n",
    "print(\"RND Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), confidence_interval))\n",
    "summary_scores.extend([confidence_interval, '', ''])\n",
    "\n",
    "parameters = {'clf__C':[0.1, 1, 10]}\n",
    "parameters['clf__loss']=('hinge','squared_hinge')\n",
    "parameters['clf__multi_class']= ('ovr', 'crammer_singer')\n",
    "parameters['clf__class_weight'] = (None, 'balanced')\n",
    "parameters['clf__tol'] = [1e-3, 1e-4]\n",
    "lsvc_cv = GridSearchCV(lsvc_clf, parameters)\n",
    "lsvc_cv.fit(X_train, y_train)\n",
    "scores = cross_val_score(lsvc_cv, X_train, y_train)\n",
    "confidence_interval = scores.std() * 2\n",
    "print(f\"LSVC best score: {lsvc_cv.best_score_}\")\n",
    "print(f\"LSVC best params: {lsvc_cv.best_params_}\")\n",
    "print(\"LSVC Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), confidence_interval))\n",
    "summary_scores.extend([confidence_interval, '',''])\n",
    "\n",
    "parameters = {'clf__alpha':[ 0.1, 1.0, 10.0]}\n",
    "parameters['clf__fit_prior']= [True, False]\n",
    "multinb_cv = GridSearchCV(multinb_clf, parameters)\n",
    "multinb_cv.fit(X_train, y_train)\n",
    "scores =  cross_val_score(multinb_cv, X_train, y_train)\n",
    "confidence_interval = scores.std() * 2\n",
    "print(f\"MNB best score: {multinb_cv.best_score_}\")\n",
    "print(f\"MNB best params: {multinb_cv.best_params_}\")\n",
    "print(\"MNB Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), confidence_interval))\n",
    "summary_scores.extend([confidence_interval, '',''])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Support</th>\n",
       "      <th>BL-P</th>\n",
       "      <th>BL-R</th>\n",
       "      <th>BL-F1</th>\n",
       "      <th>LSVC-P</th>\n",
       "      <th>LSVC-R</th>\n",
       "      <th>LSVC-F1</th>\n",
       "      <th>MNB-P</th>\n",
       "      <th>MNB-R</th>\n",
       "      <th>MNB-F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARGUMENTATIVE</td>\n",
       "      <td>309</td>\n",
       "      <td>0.493088</td>\n",
       "      <td>0.346278</td>\n",
       "      <td>0.406844</td>\n",
       "      <td>0.657343</td>\n",
       "      <td>0.608414</td>\n",
       "      <td>0.631933</td>\n",
       "      <td>0.594444</td>\n",
       "      <td>0.692557</td>\n",
       "      <td>0.639761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INTERPERSONAL</td>\n",
       "      <td>60</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.15942</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PERFORMATIVE</td>\n",
       "      <td>265</td>\n",
       "      <td>0.452736</td>\n",
       "      <td>0.343396</td>\n",
       "      <td>0.390558</td>\n",
       "      <td>0.565359</td>\n",
       "      <td>0.65283</td>\n",
       "      <td>0.605954</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.486792</td>\n",
       "      <td>0.517034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Macro</td>\n",
       "      <td>634</td>\n",
       "      <td>0.349225</td>\n",
       "      <td>0.352114</td>\n",
       "      <td>0.318941</td>\n",
       "      <td>0.717091</td>\n",
       "      <td>0.637081</td>\n",
       "      <td>0.667531</td>\n",
       "      <td>0.698575</td>\n",
       "      <td>0.604227</td>\n",
       "      <td>0.638932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CI (95\\%)</td>\n",
       "      <td></td>\n",
       "      <td>0.007185</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.023118</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.028611</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Label Support      BL-P      BL-R     BL-F1    LSVC-P    LSVC-R  \\\n",
       "0  ARGUMENTATIVE     309  0.493088  0.346278  0.406844  0.657343  0.608414   \n",
       "1  INTERPERSONAL      60  0.101852  0.366667   0.15942  0.928571      0.65   \n",
       "2   PERFORMATIVE     265  0.452736  0.343396  0.390558  0.565359   0.65283   \n",
       "3          Macro     634  0.349225  0.352114  0.318941  0.717091  0.637081   \n",
       "4      CI (95\\%)          0.007185                      0.023118             \n",
       "\n",
       "    LSVC-F1     MNB-P     MNB-R    MNB-F1  \n",
       "0  0.631933  0.594444  0.692557  0.639761  \n",
       "1  0.764706  0.950000  0.633333      0.76  \n",
       "2  0.605954  0.551282  0.486792  0.517034  \n",
       "3  0.667531  0.698575  0.604227  0.638932  \n",
       "4            0.028611                      "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "docs_test = X_test\n",
    "labels = y_test.unique()\n",
    "labels.sort()\n",
    "rds = pd.DataFrame({'Label': labels})\n",
    "macro_results = ['Macro', len(y_test)]\n",
    "\n",
    "predicted = uniform_clf.predict(docs_test)\n",
    "results = precision_recall_fscore_support(y_test, predicted)\n",
    "macro_results.extend(precision_recall_fscore_support(y_test, predicted, average='macro')[:3])\n",
    "rds['Support'] = results[3]\n",
    "rds['BL-P'] = results[0]\n",
    "rds['BL-R'] = results[1]\n",
    "rds['BL-F1'] = results[2]\n",
    "\n",
    "predicted = lsvc_cv.predict(docs_test)\n",
    "results = precision_recall_fscore_support(y_test, predicted)\n",
    "macro_results.extend(precision_recall_fscore_support(y_test, predicted, average='macro')[:3])\n",
    "rds['LSVC-P'] = results[0]\n",
    "rds['LSVC-R'] = results[1]\n",
    "rds['LSVC-F1'] = results[2]\n",
    "\n",
    "predicted = multinb_cv.predict(docs_test) \n",
    "results = precision_recall_fscore_support(y_test, predicted)\n",
    "macro_results.extend(precision_recall_fscore_support(y_test, predicted, average='macro')[:3])\n",
    "rds['MNB-P'] = results[0]\n",
    "rds['MNB-R'] = results[1]\n",
    "rds['MNB-F1'] = results[2]\n",
    "\n",
    "rds.loc[len(rds)]=macro_results\n",
    "rds.loc[len(rds)]=summary_scores\n",
    "\n",
    "rds.to_csv('output/classif_report.csv', index=False)\n",
    "rds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[214,   1,  94],\n",
       "       [ 11,  38,  11],\n",
       "       [135,   1, 129]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tengo una cuestión: en el recuadro en el que aparece el gabinete del actual presidente de la república no aparece el nombre del actual secretario de gobernación, aunque en el código de la página sí se incluyó su nombre, su nombramiento y según el código aún permanece en el cargo, pero ello no es visible en la página.' => ARGUMENTATIVE, ARGUMENTATIVE\n",
      "'hola a todos, hice una modificacion de una linea en la parte de \"tensiones con colombia\" y el problema es que no se como poner el link de la fuente..' => PERFORMATIVE, PERFORMATIVE\n"
     ]
    }
   ],
   "source": [
    "sample = X_test[:2]\n",
    "labels = y_test[:2]\n",
    "\n",
    "docs_new = sample\n",
    "predicted = lsvc_cv.predict(docs_new)\n",
    "\n",
    "for doc, label, pred in zip(docs_new, labels, predicted):\n",
    "    print('%r => %s, %s' % (doc, label, pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
